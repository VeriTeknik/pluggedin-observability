groups:
  - name: service_health
    interval: 30s
    rules:
      # Service Down Alerts
      - alert: ServiceDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} on {{ $labels.instance }} has been down for more than 2 minutes."

      # High Error Rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
            /
            sum(rate(http_requests_total[5m])) by (service)
          ) > 0.05
        for: 5m
        labels:
          severity: warning
          category: errors
        annotations:
          summary: "High error rate on {{ $labels.service }}"
          description: "{{ $labels.service }} has error rate above 5% (current: {{ $value | humanizePercentage }})"

      # Very High Error Rate
      - alert: VeryHighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
            /
            sum(rate(http_requests_total[5m])) by (service)
          ) > 0.15
        for: 2m
        labels:
          severity: critical
          category: errors
        annotations:
          summary: "Critical error rate on {{ $labels.service }}"
          description: "{{ $labels.service }} has error rate above 15% (current: {{ $value | humanizePercentage }})"

  - name: performance
    interval: 30s
    rules:
      # High Latency (p95)
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)
          ) > 2
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "High latency on {{ $labels.service }}"
          description: "{{ $labels.service }} p95 latency is above 2 seconds (current: {{ $value | humanizeDuration }})"

      # Very High Latency (p95)
      - alert: VeryHighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)
          ) > 5
        for: 2m
        labels:
          severity: critical
          category: performance
        annotations:
          summary: "Critical latency on {{ $labels.service }}"
          description: "{{ $labels.service }} p95 latency is above 5 seconds (current: {{ $value | humanizeDuration }})"

      # High Request Rate
      - alert: HighRequestRate
        expr: sum(rate(http_requests_total[5m])) by (service) > 1000
        for: 10m
        labels:
          severity: info
          category: traffic
        annotations:
          summary: "High request rate on {{ $labels.service }}"
          description: "{{ $labels.service }} is receiving more than 1000 req/s (current: {{ $value | humanize }})"

  - name: resources
    interval: 30s
    rules:
      # High Memory Usage
      - alert: HighMemoryUsage
        expr: |
          (
            node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes
          ) / node_memory_MemTotal_bytes > 0.8
        for: 5m
        labels:
          severity: warning
          category: resources
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 80% (current: {{ $value | humanizePercentage }})"

      # Critical Memory Usage
      - alert: CriticalMemoryUsage
        expr: |
          (
            node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes
          ) / node_memory_MemTotal_bytes > 0.9
        for: 2m
        labels:
          severity: critical
          category: resources
        annotations:
          summary: "Critical memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 90% (current: {{ $value | humanizePercentage }})"

      # High CPU Usage
      - alert: HighCPUUsage
        expr: |
          100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          category: resources
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 80% (current: {{ $value | humanize }}%)"

      # High Disk Usage
      - alert: HighDiskUsage
        expr: |
          (
            node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"} -
            node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"}
          ) / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"} > 0.8
        for: 5m
        labels:
          severity: warning
          category: resources
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage on {{ $labels.mountpoint }} is above 80% (current: {{ $value | humanizePercentage }})"

  - name: database
    interval: 30s
    rules:
      # PostgreSQL Down
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          category: database
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL instance {{ $labels.instance }} is down"

      # High Database Connections
      - alert: HighDatabaseConnections
        expr: |
          sum(pg_stat_activity_count) by (instance) /
          sum(pg_settings_max_connections) by (instance) > 0.8
        for: 5m
        labels:
          severity: warning
          category: database
        annotations:
          summary: "High database connection usage"
          description: "PostgreSQL connection usage is above 80% (current: {{ $value | humanizePercentage }})"

      # Slow Queries
      - alert: SlowQueries
        expr: rate(pg_stat_activity_max_tx_duration[5m]) > 60
        for: 5m
        labels:
          severity: warning
          category: database
        annotations:
          summary: "Slow database queries detected"
          description: "PostgreSQL has queries running longer than 60 seconds"

  - name: containers
    interval: 30s
    rules:
      # Container High CPU
      - alert: ContainerHighCPU
        expr: |
          sum(rate(container_cpu_usage_seconds_total{name!=""}[5m])) by (name) > 0.8
        for: 5m
        labels:
          severity: warning
          category: containers
        annotations:
          summary: "High CPU usage in container {{ $labels.name }}"
          description: "Container {{ $labels.name }} CPU usage is high (current: {{ $value | humanize }})"

      # Container High Memory
      - alert: ContainerHighMemory
        expr: |
          container_memory_usage_bytes{name!=""} / container_spec_memory_limit_bytes{name!=""} > 0.8
        for: 5m
        labels:
          severity: warning
          category: containers
        annotations:
          summary: "High memory usage in container {{ $labels.name }}"
          description: "Container {{ $labels.name }} memory usage is above 80%"

      # Container Restarting
      - alert: ContainerRestarting
        expr: rate(container_last_seen{name!=""}[5m]) > 0
        for: 5m
        labels:
          severity: warning
          category: containers
        annotations:
          summary: "Container {{ $labels.name }} is restarting"
          description: "Container {{ $labels.name }} has restarted {{ $value }} times in the last 5 minutes"

  - name: milvus
    interval: 30s
    rules:
      # Milvus Down
      - alert: MilvusDown
        expr: up{job="milvus"} == 0
        for: 2m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "Milvus is down"
          description: "Milvus instance {{ $labels.instance }} has been down for more than 2 minutes."

      # High Search Latency
      - alert: MilvusHighSearchLatency
        expr: |
          histogram_quantile(0.95,
            rate(milvus_search_latency_seconds_bucket[5m])
          ) > 2
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "High Milvus search latency"
          description: "Milvus p95 search latency is above 2 seconds (current: {{ $value | humanizeDuration }})"

      # Low Cache Hit Ratio
      - alert: MilvusLowCacheHitRatio
        expr: milvus_cache_hit_ratio < 0.8
        for: 10m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "Low Milvus cache hit ratio"
          description: "Milvus cache hit ratio is below 80% (current: {{ $value | humanizePercentage }})"

      # High Memory Usage
      - alert: MilvusHighMemory
        expr: |
          process_resident_memory_bytes{job="milvus"} /
          (1024 * 1024 * 1024) > 8
        for: 5m
        labels:
          severity: warning
          category: resources
        annotations:
          summary: "High Milvus memory usage"
          description: "Milvus is using more than 8GB memory (current: {{ $value | humanize }}GB)"

  - name: rag_api
    interval: 30s
    rules:
      # RAG API Server Down
      - alert: RagApiServerDown
        expr: up{job="api-server"} == 0
        for: 2m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "RAG API Server is down"
          description: "api.plugged.in has been down for more than 2 minutes."

      # High RAG Query Error Rate
      - alert: HighRagQueryErrorRate
        expr: |
          (
            sum(rate(rag_queries_total{status="error"}[5m]))
            /
            sum(rate(rag_queries_total[5m]))
          ) > 0.1
        for: 5m
        labels:
          severity: warning
          category: rag
        annotations:
          summary: "High RAG query error rate"
          description: "RAG query error rate is above 10% (current: {{ $value | humanizePercentage }})"

      # Critical RAG Query Error Rate
      - alert: CriticalRagQueryErrorRate
        expr: |
          (
            sum(rate(rag_queries_total{status="error"}[5m]))
            /
            sum(rate(rag_queries_total[5m]))
          ) > 0.25
        for: 2m
        labels:
          severity: critical
          category: rag
        annotations:
          summary: "Critical RAG query error rate"
          description: "RAG query error rate is above 25% (current: {{ $value | humanizePercentage }})"

      # Slow RAG Queries
      - alert: SlowRagQueries
        expr: |
          histogram_quantile(0.95,
            sum(rate(rag_query_duration_seconds_bucket[5m])) by (le)
          ) > 10
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "Slow RAG queries"
          description: "p95 RAG query duration is above 10 seconds (current: {{ $value | humanizeDuration }})"

      # High Document Processing Failures
      - alert: HighDocumentProcessingFailures
        expr: |
          (
            sum(rate(document_processing_duration_seconds_count{status="error"}[10m]))
            /
            sum(rate(document_processing_duration_seconds_count[10m]))
          ) > 0.2
        for: 5m
        labels:
          severity: warning
          category: rag
        annotations:
          summary: "High document processing failure rate"
          description: "Document processing error rate is above 20% (current: {{ $value | humanizePercentage }})"

      # Slow Vector Search
      - alert: SlowVectorSearch
        expr: |
          histogram_quantile(0.95,
            sum(rate(vector_search_duration_seconds_bucket{collection="document_embeddings"}[5m])) by (le)
          ) > 5
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "Slow vector search operations"
          description: "p95 vector search duration is above 5 seconds (current: {{ $value | humanizeDuration }})"

      # High LLM API Error Rate
      - alert: HighLlmApiErrorRate
        expr: |
          (
            sum(rate(llm_api_calls_total{status="error"}[5m])) by (provider)
            /
            sum(rate(llm_api_calls_total[5m])) by (provider)
          ) > 0.1
        for: 5m
        labels:
          severity: warning
          category: llm
        annotations:
          summary: "High LLM API error rate for {{ $labels.provider }}"
          description: "{{ $labels.provider }} API error rate is above 10% (current: {{ $value | humanizePercentage }})"

      # Slow LLM API Calls
      - alert: SlowLlmApiCalls
        expr: |
          histogram_quantile(0.95,
            sum(rate(llm_api_duration_seconds_bucket[5m])) by (le, provider)
          ) > 30
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "Slow LLM API calls for {{ $labels.provider }}"
          description: "p95 {{ $labels.provider }} API call duration is above 30 seconds (current: {{ $value | humanizeDuration }})"

      # No RAG Queries
      - alert: NoRagQueries
        expr: |
          sum(rate(rag_queries_total[5m])) == 0
        for: 15m
        labels:
          severity: info
          category: rag
        annotations:
          summary: "No RAG queries received"
          description: "RAG API has not received any queries in the last 15 minutes"

  - name: registry_proxy
    interval: 30s
    rules:
      # Registry Proxy Down
      - alert: RegistryProxyDown
        expr: up{job="registry-proxy"} == 0
        for: 2m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "Registry Proxy is down"
          description: "registry.plugged.in has been down for more than 2 minutes."

      # High Registry Proxy Error Rate
      - alert: RegistryProxyHighErrorRate
        expr: |
          (
            sum(rate(registry_proxy_http_requests_total{service="registry-proxy",status=~"5.."}[5m]))
            /
            sum(rate(registry_proxy_http_requests_total{service="registry-proxy"}[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: warning
          category: errors
        annotations:
          summary: "High error rate on Registry Proxy"
          description: "Registry Proxy error rate is above 5% (current: {{ $value | humanizePercentage }})"

      # Critical Registry Proxy Error Rate
      - alert: RegistryProxyCriticalErrorRate
        expr: |
          (
            sum(rate(registry_proxy_http_requests_total{service="registry-proxy",status=~"5.."}[5m]))
            /
            sum(rate(registry_proxy_http_requests_total{service="registry-proxy"}[5m]))
          ) > 0.15
        for: 2m
        labels:
          severity: critical
          category: errors
        annotations:
          summary: "Critical error rate on Registry Proxy"
          description: "Registry Proxy error rate is above 15% (current: {{ $value | humanizePercentage }})"

      # High Registry Proxy Latency
      - alert: RegistryProxyHighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(registry_proxy_http_request_duration_seconds_bucket{service="registry-proxy"}[5m])) by (le)
          ) > 2
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "High latency on Registry Proxy"
          description: "Registry Proxy p95 latency is above 2 seconds (current: {{ $value | humanizeDuration }})"

      # Critical Registry Proxy Latency
      - alert: RegistryProxyCriticalLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(registry_proxy_http_request_duration_seconds_bucket{service="registry-proxy"}[5m])) by (le)
          ) > 5
        for: 2m
        labels:
          severity: critical
          category: performance
        annotations:
          summary: "Critical latency on Registry Proxy"
          description: "Registry Proxy p95 latency is above 5 seconds (current: {{ $value | humanizeDuration }})"

      # Low Cache Hit Rate
      - alert: RegistryProxyLowCacheHitRate
        expr: |
          (
            sum(rate(registry_proxy_cache_hits_total{service="registry-proxy"}[5m]))
            /
            (sum(rate(registry_proxy_cache_hits_total{service="registry-proxy"}[5m])) + sum(rate(registry_proxy_cache_misses_total{service="registry-proxy"}[5m])))
          ) < 0.7
        for: 10m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "Low cache hit rate on Registry Proxy"
          description: "Registry Proxy cache hit rate is below 70% (current: {{ $value | humanizePercentage }})"

      # High Database Connections
      - alert: RegistryProxyHighDatabaseConnections
        expr: registry_proxy_database_connections_active{service="registry-proxy"} > 80
        for: 5m
        labels:
          severity: warning
          category: database
        annotations:
          summary: "High database connections from Registry Proxy"
          description: "Registry Proxy has {{ $value }} active database connections (threshold: 80)"

      # Slow Database Queries
      - alert: RegistryProxySlowDatabaseQueries
        expr: |
          histogram_quantile(0.95,
            sum(rate(registry_proxy_database_query_duration_seconds_bucket{service="registry-proxy"}[5m])) by (le, operation)
          ) > 1
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "Slow database queries on Registry Proxy"
          description: "Registry Proxy p95 database query duration is above 1 second for {{ $labels.operation }} (current: {{ $value | humanizeDuration }})"

      # High Upstream Error Rate
      - alert: RegistryProxyHighUpstreamErrorRate
        expr: |
          (
            sum(rate(registry_proxy_upstream_requests_total{service="registry-proxy",status=~"5.."}[5m]))
            /
            sum(rate(registry_proxy_upstream_requests_total{service="registry-proxy"}[5m]))
          ) > 0.1
        for: 5m
        labels:
          severity: warning
          category: upstream
        annotations:
          summary: "High upstream error rate on Registry Proxy"
          description: "Registry Proxy upstream error rate is above 10% (current: {{ $value | humanizePercentage }})"

      # Enhanced Endpoint 500 Errors
      - alert: EnhancedEndpoint500Errors
        expr: |
          sum(rate(registry_proxy_http_requests_total{service="registry-proxy",endpoint="/v0/enhanced/servers",status="500"}[5m])) > 0
        for: 2m
        labels:
          severity: critical
          category: errors
        annotations:
          summary: "500 errors on Enhanced Servers endpoint"
          description: "The /v0/enhanced/servers endpoint is returning 500 errors ({{ $value | humanize }} errors/sec). Check database query errors in logs."

      # High Error Count from Logs
      - alert: HighErrorLogCount
        expr: |
          sum(count_over_time({service="registry-proxy", level="error"}[5m])) > 10
        for: 3m
        labels:
          severity: warning
          category: errors
        annotations:
          summary: "High error log count on Registry Proxy"
          description: "Registry Proxy has logged {{ $value }} errors in the last 5 minutes. Check Loki logs for details."

      # Database Query Errors
      - alert: DatabaseQueryErrors
        expr: |
          sum(count_over_time({service="registry-proxy", level="error"} |= "Failed to query enhanced servers"[5m])) > 5
        for: 2m
        labels:
          severity: critical
          category: database
        annotations:
          summary: "Database query failures on Registry Proxy"
          description: "Registry Proxy has {{ $value }} database query failures in the last 5 minutes. Check database connectivity and query syntax."

      # Array Parameter Errors (our specific fix)
      - alert: ArrayParameterErrors
        expr: |
          sum(count_over_time({service="registry-proxy", level="error"} |= "pq: op ANY/ALL (array) requires array on right side"[5m])) > 0
        for: 1m
        labels:
          severity: critical
          category: database
        annotations:
          summary: "PostgreSQL array parameter errors detected"
          description: "Registry Proxy is experiencing PostgreSQL array parameter errors. This indicates the query builder fix may not have been applied correctly."

      # Filter Parameter Errors
      - alert: FilterParameterErrors
        expr: |
          sum(count_over_time({service="registry-proxy", level="error", endpoint="/v0/enhanced/servers"} |~ "registry_types|transports|tags"[5m])) > 5
        for: 2m
        labels:
          severity: warning
          category: errors
        annotations:
          summary: "Filter parameter errors on Enhanced endpoint"
          description: "The /v0/enhanced/servers endpoint is experiencing {{ $value }} filter-related errors. Check registry_types, transports, and tags parameters."